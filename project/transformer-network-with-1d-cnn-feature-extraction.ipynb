{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_kg_hide-input": false,
    "_uuid": "e1baa7518f18a7ff1f2767df1b29c051955502ff"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from os import listdir, makedirs\n",
    "from os.path import isfile, join, basename, splitext, isfile, exists\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "\n",
    "import keras\n",
    "from keras.engine.input_layer import Input\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import random, os, sys\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.callbacks import *\n",
    "from keras.initializers import *\n",
    "import tensorflow as tf\n",
    "from keras.engine.topology import Layer\n",
    "\n",
    "pd.set_option('precision', 30)\n",
    "np.set_printoptions(precision = 30)\n",
    "\n",
    "np.random.seed(368)\n",
    "tf.set_random_seed(368)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "ccfffa3051a10a0eb8c74527bd8d788c49a340f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 6s, sys: 9.98 s, total: 2min 16s\n",
      "Wall time: 2min 17s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "train_df = pd.read_csv('../input/train.csv', dtype={'acoustic_data': np.int8, 'time_to_failure': np.float32})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_kg_hide-input": false,
    "_kg_hide-output": false,
    "_uuid": "30679acc658f8437d3126d4353068f7d40588a29"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acoustic_data</th>\n",
       "      <th>time_to_failure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12</td>\n",
       "      <td>1.46909999847412109375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>1.46909999847412109375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>1.46909999847412109375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>1.46909999847412109375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>1.46909999847412109375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   acoustic_data         time_to_failure\n",
       "0             12  1.46909999847412109375\n",
       "1              6  1.46909999847412109375\n",
       "2              8  1.46909999847412109375\n",
       "3              5  1.46909999847412109375\n",
       "4              8  1.46909999847412109375"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "693ff1759a78bcc26123e47b011355b54a8ac6ab"
   },
   "outputs": [],
   "source": [
    "X_train = train_df.acoustic_data.values\n",
    "y_train = train_df.time_to_failure.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "10f0836f9ac55fd57430633125ee33293b8c3bab"
   },
   "source": [
    "Find complete segments in the training data (time to failure goes to zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "f0f90d3483c4349050cb3e9ada89cfaf974bb0c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 5656573), (5656573, 50085877), (50085877, 104677355), (104677355, 138772452), (138772452, 187641819), (187641819, 218652629), (218652629, 245829584), (245829584, 307838916), (307838916, 338276286), (338276286, 375377847), (375377847, 419368879), (419368879, 461811622), (461811622, 495800224), (495800224, 528777114), (528777114, 585568143), (585568143, 621985672)]\n"
     ]
    }
   ],
   "source": [
    "ends_mask = np.less(y_train[:-1], y_train[1:])\n",
    "segment_ends = np.nonzero(ends_mask)\n",
    "\n",
    "train_segments = []\n",
    "start = 0\n",
    "for end in segment_ends[0]:\n",
    "    train_segments.append((start, end))\n",
    "    start = end\n",
    "    \n",
    "print(train_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "2e1194a9e1726095db78e14183c971a1dcbc89a1"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Segment sizes')\n",
    "_ = plt.bar(np.arange(len(train_segments)), [ s[1] - s[0] for s in train_segments])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5ad80d3483b09c18d07bb62229a802d2284cd0e3"
   },
   "source": [
    "The generator samples randomly from the segmens without crossing the boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "389b43c4107c58029e048522b9b29a38662769c8"
   },
   "outputs": [],
   "source": [
    "class EarthQuakeRandom(keras.utils.Sequence):\n",
    "\n",
    "    def __init__(self, x, y, x_mean, x_std, segments, ts_length, batch_size, steps_per_epoch):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.segments = segments\n",
    "        self.ts_length = ts_length\n",
    "        self.batch_size = batch_size\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "        self.segments_size = np.array([s[1] - s[0] for s in segments])\n",
    "        self.segments_p = self.segments_size / self.segments_size.sum()\n",
    "        self.x_mean = x_mean\n",
    "        self.x_std = x_std\n",
    "\n",
    "    def get_batch_size(self):\n",
    "        return self.batch_size\n",
    "\n",
    "    def get_ts_length(self):\n",
    "        return self.ts_length\n",
    "\n",
    "    def get_segments(self):\n",
    "        return self.segments\n",
    "\n",
    "    def get_segments_p(self):\n",
    "        return self.segments_p\n",
    "\n",
    "    def get_segments_size(self):\n",
    "        return self.segments_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.steps_per_epoch\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        segment_index = np.random.choice(range(len(self.segments)), p=self.segments_p)\n",
    "        segment = self.segments[segment_index]\n",
    "        end_indexes = np.random.randint(segment[0] + self.ts_length, segment[1], size=self.batch_size)\n",
    "\n",
    "        x_batch = np.empty((self.batch_size, self.ts_length))\n",
    "        y_batch = np.empty(self.batch_size, )\n",
    "\n",
    "        for i, end in enumerate(end_indexes):\n",
    "            x_batch[i, :] = self.x[end - self.ts_length: end]\n",
    "            y_batch[i] = self.y[end - 1]\n",
    "            \n",
    "        x_batch = (x_batch - self.x_mean)/self.x_std\n",
    "\n",
    "        return np.expand_dims(x_batch, axis=2), y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d7985fae8438678b6542b73dc80b64ceb35bde22"
   },
   "source": [
    "We could use any segments for training / validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "a2287d88c18d90e7f1cf41917265698354b48624"
   },
   "outputs": [],
   "source": [
    "t_segments = [train_segments[i] for i in [ 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]]\n",
    "v_segments = [train_segments[i] for i in [ 0, 1, 2, 3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4c2ed95db99225cf1e8226eee68114f31f982c6a"
   },
   "source": [
    "I think it does not make big difference but lets not leak into the validation data and calculate mean and standrad deviation on the training data only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "fa319e6bdca9dd9866856d514fec8ac093cf818d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.472289301190891 6.189013535612676\n"
     ]
    }
   ],
   "source": [
    "x_sum = 0.\n",
    "count = 0\n",
    "\n",
    "for s in t_segments:\n",
    "    x_sum += X_train[s[0]:s[1]].sum()\n",
    "    count += (s[1] - s[0])\n",
    "\n",
    "X_train_mean = x_sum/count\n",
    "\n",
    "x2_sum = 0.\n",
    "for s in t_segments:\n",
    "    x2_sum += np.power(X_train[s[0]:s[1]] - X_train_mean, 2).sum()\n",
    "\n",
    "X_train_std =  np.sqrt(x2_sum/count)\n",
    "\n",
    "print(X_train_mean, X_train_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "5cae20837d7405650f9b09891e5e71a156abf883"
   },
   "outputs": [],
   "source": [
    "train_gen = EarthQuakeRandom(\n",
    "    x = X_train, \n",
    "    y = y_train,\n",
    "    x_mean = X_train_mean, \n",
    "    x_std = X_train_std,\n",
    "    segments = t_segments,\n",
    "    ts_length = 150000,\n",
    "    batch_size = 64,\n",
    "    steps_per_epoch = 400\n",
    ")\n",
    "\n",
    "valid_gen = EarthQuakeRandom(\n",
    "    x = X_train, \n",
    "    y = y_train,\n",
    "    x_mean = X_train_mean, \n",
    "    x_std = X_train_std,\n",
    "    segments = v_segments,\n",
    "    ts_length = 150000,\n",
    "    batch_size = 64,\n",
    "    steps_per_epoch = 400\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.EarthQuakeRandom object at 0x7f9050dd1ef0>\n"
     ]
    }
   ],
   "source": [
    "print(train_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "70ab7209fe41fac92e9ed89ed0617ce383bd0453"
   },
   "source": [
    "Use convolutional layers to learn the features and reduce the time sequence length "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/shujian/transformer-with-lstm\n",
    "\n",
    "try:\n",
    "    from dataloader import TokenList, pad_to_longest\n",
    "    # for transformer\n",
    "except: pass\n",
    "\n",
    "\n",
    "\n",
    "embed_size = 60\n",
    "\n",
    "class LayerNormalization(Layer):\n",
    "    def __init__(self, eps=1e-6, **kwargs):\n",
    "        self.eps = eps\n",
    "        super(LayerNormalization, self).__init__(**kwargs)\n",
    "    def build(self, input_shape):\n",
    "        self.gamma = self.add_weight(name='gamma', shape=input_shape[-1:],\n",
    "                                     initializer=Ones(), trainable=True)\n",
    "        self.beta = self.add_weight(name='beta', shape=input_shape[-1:],\n",
    "                                    initializer=Zeros(), trainable=True)\n",
    "        super(LayerNormalization, self).build(input_shape)\n",
    "    def call(self, x):\n",
    "        mean = K.mean(x, axis=-1, keepdims=True)\n",
    "        std = K.std(x, axis=-1, keepdims=True)\n",
    "        return self.gamma * (x - mean) / (std + self.eps) + self.beta\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "class ScaledDotProductAttention():\n",
    "    def __init__(self, d_model, attn_dropout=0.1):\n",
    "        self.temper = np.sqrt(d_model)\n",
    "        self.dropout = Dropout(attn_dropout)\n",
    "    def __call__(self, q, k, v, mask):\n",
    "        attn = Lambda(lambda x:K.batch_dot(x[0],x[1],axes=[2,2])/self.temper)([q, k])\n",
    "        if mask is not None:\n",
    "            mmask = Lambda(lambda x:(-1e+10)*(1-x))(mask)\n",
    "            attn = Add()([attn, mmask])\n",
    "        attn = Activation('softmax')(attn)\n",
    "        attn = self.dropout(attn)\n",
    "        output = Lambda(lambda x:K.batch_dot(x[0], x[1]))([attn, v])\n",
    "        return output, attn\n",
    "\n",
    "class MultiHeadAttention():\n",
    "    # mode 0 - big martixes, faster; mode 1 - more clear implementation\n",
    "    def __init__(self, n_head, d_model, d_k, d_v, dropout, mode=0, use_norm=True):\n",
    "        self.mode = mode\n",
    "        self.n_head = n_head\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.dropout = dropout\n",
    "        if mode == 0:\n",
    "            self.qs_layer = Dense(n_head*d_k, use_bias=False)\n",
    "            self.ks_layer = Dense(n_head*d_k, use_bias=False)\n",
    "            self.vs_layer = Dense(n_head*d_v, use_bias=False)\n",
    "        elif mode == 1:\n",
    "            self.qs_layers = []\n",
    "            self.ks_layers = []\n",
    "            self.vs_layers = []\n",
    "            for _ in range(n_head):\n",
    "                self.qs_layers.append(TimeDistributed(Dense(d_k, use_bias=False)))\n",
    "                self.ks_layers.append(TimeDistributed(Dense(d_k, use_bias=False)))\n",
    "                self.vs_layers.append(TimeDistributed(Dense(d_v, use_bias=False)))\n",
    "        self.attention = ScaledDotProductAttention(d_model)\n",
    "        self.layer_norm = LayerNormalization() if use_norm else None\n",
    "        self.w_o = TimeDistributed(Dense(d_model))\n",
    "\n",
    "    def __call__(self, q, k, v, mask=None):\n",
    "        d_k, d_v = self.d_k, self.d_v\n",
    "        n_head = self.n_head\n",
    "\n",
    "        if self.mode == 0:\n",
    "            qs = self.qs_layer(q)  # [batch_size, len_q, n_head*d_k]\n",
    "            ks = self.ks_layer(k)\n",
    "            vs = self.vs_layer(v)\n",
    "\n",
    "            def reshape1(x):\n",
    "                s = tf.shape(x)   # [batch_size, len_q, n_head * d_k]\n",
    "                x = tf.reshape(x, [s[0], s[1], n_head, d_k])\n",
    "                x = tf.transpose(x, [2, 0, 1, 3])  \n",
    "                x = tf.reshape(x, [-1, s[1], d_k])  # [n_head * batch_size, len_q, d_k]\n",
    "                return x\n",
    "            qs = Lambda(reshape1)(qs)\n",
    "            ks = Lambda(reshape1)(ks)\n",
    "            vs = Lambda(reshape1)(vs)\n",
    "\n",
    "            if mask is not None:\n",
    "                mask = Lambda(lambda x:K.repeat_elements(x, n_head, 0))(mask)\n",
    "            head, attn = self.attention(qs, ks, vs, mask=mask)  \n",
    "                \n",
    "            def reshape2(x):\n",
    "                s = tf.shape(x)   # [n_head * batch_size, len_v, d_v]\n",
    "                x = tf.reshape(x, [n_head, -1, s[1], s[2]]) \n",
    "                x = tf.transpose(x, [1, 2, 0, 3])\n",
    "                x = tf.reshape(x, [-1, s[1], n_head*d_v])  # [batch_size, len_v, n_head * d_v]\n",
    "                return x\n",
    "            head = Lambda(reshape2)(head)\n",
    "        elif self.mode == 1:\n",
    "            heads = []; attns = []\n",
    "            for i in range(n_head):\n",
    "                qs = self.qs_layers[i](q)   \n",
    "                ks = self.ks_layers[i](k) \n",
    "                vs = self.vs_layers[i](v) \n",
    "                head, attn = self.attention(qs, ks, vs, mask)\n",
    "                heads.append(head); attns.append(attn)\n",
    "            head = Concatenate()(heads) if n_head > 1 else heads[0]\n",
    "            attn = Concatenate()(attns) if n_head > 1 else attns[0]\n",
    "\n",
    "        outputs = self.w_o(head)\n",
    "        outputs = Dropout(self.dropout)(outputs)\n",
    "        if not self.layer_norm: return outputs, attn\n",
    "        # outputs = Add()([outputs, q]) # sl: fix\n",
    "        return self.layer_norm(outputs), attn\n",
    "\n",
    "class PositionwiseFeedForward():\n",
    "    def __init__(self, d_hid, d_inner_hid, dropout=0.1):\n",
    "        self.w_1 = Conv1D(d_inner_hid, 1, activation='relu')\n",
    "        self.w_2 = Conv1D(d_hid, 1)\n",
    "        self.layer_norm = LayerNormalization()\n",
    "        self.dropout = Dropout(dropout)\n",
    "    def __call__(self, x):\n",
    "        output = self.w_1(x) \n",
    "        output = self.w_2(output)\n",
    "        output = self.dropout(output)\n",
    "        output = Add()([output, x])\n",
    "        return self.layer_norm(output)\n",
    "\n",
    "class EncoderLayer():\n",
    "    def __init__(self, d_model, d_inner_hid, n_head, d_k, d_v, dropout=0.1):\n",
    "        self.self_att_layer = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)\n",
    "        self.pos_ffn_layer  = PositionwiseFeedForward(d_model, d_inner_hid, dropout=dropout)\n",
    "    def __call__(self, enc_input, mask=None):\n",
    "        output, slf_attn = self.self_att_layer(enc_input, enc_input, enc_input, mask=mask)\n",
    "        output = self.pos_ffn_layer(output)\n",
    "        return output, slf_attn\n",
    "\n",
    "\n",
    "def GetPosEncodingMatrix(max_len, d_emb):\n",
    "    pos_enc = np.array([\n",
    "        [pos / np.power(10000, 2 * (j // 2) / d_emb) for j in range(d_emb)] \n",
    "        if pos != 0 else np.zeros(d_emb) \n",
    "            for pos in range(max_len)\n",
    "            ])\n",
    "    pos_enc[1:, 0::2] = np.sin(pos_enc[1:, 0::2]) # dim 2i\n",
    "    pos_enc[1:, 1::2] = np.cos(pos_enc[1:, 1::2]) # dim 2i+1\n",
    "    return pos_enc\n",
    "\n",
    "def GetPadMask(q, k):\n",
    "    ones = K.expand_dims(K.ones_like(q, 'float32'), -1)\n",
    "    mask = K.cast(K.expand_dims(K.not_equal(k, 0), 1), 'float32')\n",
    "    mask = K.batch_dot(ones, mask, axes=[2,1])\n",
    "    return mask\n",
    "\n",
    "def GetSubMask(s):\n",
    "    len_s = tf.shape(s)[1]\n",
    "    bs = tf.shape(s)[:1]\n",
    "    mask = K.cumsum(tf.eye(len_s, batch_shape=bs), 1)\n",
    "    return mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CnnTransformerModel():\n",
    "    i = Input(shape = (150000, 1))\n",
    "    \n",
    "    x = Convolution1D( 32, kernel_size = 10, strides = 10, activation='relu')(i)\n",
    "    x = Convolution1D(32, kernel_size = 10, strides = 10, activation='relu')(x)\n",
    "    x = Convolution1D(64, kernel_size = 10, strides = 10, activation='relu')(x)\n",
    "   # x = (CuDNNLSTM(16, return_sequences = True, return_state = False))(x)\n",
    "    x, slf_attn = MultiHeadAttention(n_head=5, d_model=300, d_k=64, d_v=64, dropout=0.3)(x, x, x)\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    \n",
    "    y = Dense(1)(avg_pool)\n",
    "    \n",
    "\n",
    "    return Model(inputs = [i], outputs = [y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_kg_hide-input": false,
    "_kg_hide-output": false,
    "_uuid": "78f8084ea9e762c4e59bd933abea41704ad99bdd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input 0 is incompatible with layer lstm_2: expected ndim=3, found ndim=2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-cb9b16f8de49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m150000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m250\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mean_absolute_error'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m         \u001b[0;31m# If any of `initial_state` or `constants` are specified and are Keras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    412\u001b[0m                 \u001b[0;31m# Raise exceptions in case the input is not compatible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m                 \u001b[0;31m# with the input_spec specified in the layer constructor.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_input_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m                 \u001b[0;31m# Collect input shapes to build layer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                                      \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m': expected ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                                      \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', found ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m                                      str(K.ndim(x)))\n\u001b[0m\u001b[1;32m    312\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_ndim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m                 \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input 0 is incompatible with layer lstm_2: expected ndim=3, found ndim=2"
     ]
    }
   ],
   "source": [
    "# model = CnnTransformerModel()\n",
    "inp = Input(shape=(150000,1))\n",
    "model = LSTM(150)(inp)\n",
    "model = LSTM(250)(model)\n",
    "model.compile(loss='mean_absolute_error', optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5bed51171db8bf8c5e1731119fd31205bfd80a82"
   },
   "source": [
    "Train the model with early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_kg_hide-input": false,
    "_kg_hide-output": false,
    "_uuid": "e1061c7cde0687450300f516735aad0cc5dbf08f"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'fit_generator'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-af4e2d629503>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m hist = model.fit_generator(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mgenerator\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mtrain_gen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m150\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'fit_generator'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "hist = model.fit_generator(\n",
    "    generator =  train_gen,\n",
    "    epochs = 150, \n",
    "    verbose = 1, \n",
    "    validation_data = valid_gen,\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience = 150, verbose = 1)\n",
    "    ]\n",
    ")\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "9b4790925984514e64ca5a9b46de8b309062e0cf"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-91b795c1f0b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Model loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hist' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "_= plt.legend(['Train', 'Test'], loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "322"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "del train_gen\n",
    "del valid_gen\n",
    "del X_train\n",
    "del y_train\n",
    "del train_df\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'save_weights'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-e2aa1136d692>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./trained_model.h5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'save_weights'"
     ]
    }
   ],
   "source": [
    "model.save_weights('./trained_model.h5', overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c3340d6d6ce75585f90c98f1728b1cd664d7f33f"
   },
   "source": [
    "Load and normalize the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test(ts_length = 150000):\n",
    "    base_dir = '../input/test/'\n",
    "    test_files = [f for f in listdir(base_dir) if isfile(join(base_dir, f))]\n",
    "\n",
    "    ts = np.empty([len(test_files), ts_length])\n",
    "    ids = []\n",
    "    \n",
    "    i = 0\n",
    "    for f in tqdm_notebook(test_files):\n",
    "        ids.append(splitext(f)[0])\n",
    "        t_df = pd.read_csv(base_dir + f, dtype={\"acoustic_data\": np.int8})\n",
    "        ts[i, :] = t_df['acoustic_data'].values\n",
    "        i = i + 1\n",
    "\n",
    "    return ts, ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "0f005579ea08913f4f68a3749bd761df6cef2b1b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd9c1e81fb63413fbcc246302357d611",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2624), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_data, test_ids = load_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "0c3b7a864a9f53af142a08883def46c3866c5464"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2624, 150000, 1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = ((test_data - X_train_mean)/ X_train_std)\n",
    "X_test = np.expand_dims(X_test, 2)\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bf9b36929e5228d4d94b3b7ad1b9011bf088ac44"
   },
   "source": [
    "Load best model and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "435449fda2bf96635e67d69f56227e140c4cea99"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-33f45f2977c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "9aaf9fb44edba5879a75c68820527d9180d2b3c6"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-3029fd4a3359>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msubmission_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'seg_id'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtest_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'time_to_failure'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'y_pred' is not defined"
     ]
    }
   ],
   "source": [
    "submission_df = pd.DataFrame({'seg_id': test_ids, 'time_to_failure': y_pred[:, 0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "9b9d5c63161f637de2e39b59e8e4d7c2f3049581"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'submission_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-2fba80f9d7ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msubmission_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"submission.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'submission_df' is not defined"
     ]
    }
   ],
   "source": [
    "submission_df.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
