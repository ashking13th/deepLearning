{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "val_x, val_y = mnist.test.next_batch(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 cost = 12.31493\n",
      "Epoch: 2 cost = 2.45579\n",
      "Epoch: 3 cost = 1.65941\n",
      "Epoch: 4 cost = 1.32675\n",
      "Epoch: 5 cost = 1.10262\n",
      "Epoch: 6 cost = 0.94012\n",
      "Epoch: 7 cost = 0.83682\n",
      "Epoch: 8 cost = 0.77915\n",
      "Epoch: 9 cost = 0.70379\n",
      "Epoch: 10 cost = 0.64988\n",
      "Epoch: 11 cost = 0.59671\n",
      "Epoch: 12 cost = 0.55405\n",
      "Epoch: 13 cost = 0.52694\n",
      "Epoch: 14 cost = 0.53662\n",
      "Epoch: 15 cost = 0.51040\n",
      "Epoch: 16 cost = 0.46630\n",
      "Epoch: 17 cost = 0.45575\n",
      "Epoch: 18 cost = 0.43291\n",
      "Epoch: 19 cost = 0.44250\n",
      "Epoch: 20 cost = 0.41412\n",
      "Epoch: 21 cost = 0.37686\n",
      "Epoch: 22 cost = 0.39699\n",
      "Epoch: 23 cost = 0.37654\n",
      "Epoch: 24 cost = 0.37620\n",
      "Epoch: 25 cost = 0.35740\n",
      "Epoch: 26 cost = 0.36356\n",
      "Epoch: 27 cost = 0.34075\n",
      "Epoch: 28 cost = 0.33232\n",
      "Epoch: 29 cost = 0.31296\n",
      "Epoch: 30 cost = 0.33078\n",
      "Epoch: 31 cost = 0.32812\n",
      "Epoch: 32 cost = 0.31706\n",
      "Epoch: 33 cost = 0.29235\n",
      "Epoch: 34 cost = 0.30454\n",
      "Epoch: 35 cost = 0.29186\n",
      "Epoch: 36 cost = 0.28761\n",
      "Epoch: 37 cost = 0.27916\n",
      "Epoch: 38 cost = 0.28150\n",
      "Epoch: 39 cost = 0.28000\n",
      "Epoch: 40 cost = 0.27934\n",
      "Epoch: 41 cost = 0.26780\n",
      "Epoch: 42 cost = 0.26383\n",
      "Epoch: 43 cost = 0.26226\n",
      "Epoch: 44 cost = 0.26199\n",
      "Epoch: 45 cost = 0.25780\n",
      "Epoch: 46 cost = 0.23992\n",
      "Epoch: 47 cost = 0.25170\n",
      "Epoch: 48 cost = 0.25147\n",
      "Epoch: 49 cost = 0.24010\n",
      "Epoch: 50 cost = 0.24703\n",
      "Epoch: 51 cost = 0.23231\n",
      "Epoch: 52 cost = 0.23652\n",
      "Epoch: 53 cost = 0.23403\n",
      "Epoch: 54 cost = 0.23483\n",
      "Epoch: 55 cost = 0.24199\n",
      "Epoch: 56 cost = 0.22991\n",
      "Epoch: 57 cost = 0.21546\n",
      "Epoch: 58 cost = 0.21961\n",
      "Epoch: 59 cost = 0.22234\n",
      "Epoch: 60 cost = 0.23606\n",
      "Epoch: 61 cost = 0.21078\n",
      "Epoch: 62 cost = 0.20792\n",
      "Epoch: 63 cost = 0.22377\n",
      "Epoch: 64 cost = 0.22699\n",
      "Epoch: 65 cost = 0.21167\n",
      "Epoch: 66 cost = 0.20968\n",
      "Epoch: 67 cost = 0.21172\n",
      "Epoch: 68 cost = 0.21264\n",
      "Epoch: 69 cost = 0.21619\n",
      "Epoch: 70 cost = 0.20073\n",
      "Epoch: 71 cost = 0.20037\n",
      "Epoch: 72 cost = 0.19988\n",
      "Epoch: 73 cost = 0.20315\n",
      "Epoch: 74 cost = 0.20991\n",
      "Epoch: 75 cost = 0.20834\n",
      "Epoch: 76 cost = 0.17369\n",
      "Epoch: 77 cost = 0.20719\n",
      "Epoch: 78 cost = 0.20061\n",
      "Epoch: 79 cost = 0.20673\n",
      "Epoch: 80 cost = 0.20486\n",
      "Epoch: 81 cost = 0.18790\n",
      "Epoch: 82 cost = 0.18770\n",
      "Epoch: 83 cost = 0.20116\n",
      "Epoch: 84 cost = 0.18198\n",
      "Epoch: 85 cost = 0.20866\n",
      "Epoch: 86 cost = 0.18424\n",
      "Epoch: 87 cost = 0.19544\n",
      "Epoch: 88 cost = 0.20096\n",
      "Epoch: 89 cost = 0.18871\n",
      "Epoch: 90 cost = 0.18091\n",
      "Epoch: 91 cost = 0.17862\n",
      "Epoch: 92 cost = 0.18279\n",
      "Epoch: 93 cost = 0.18696\n",
      "Epoch: 94 cost = 0.17739\n",
      "Epoch: 95 cost = 0.18238\n",
      "Epoch: 96 cost = 0.17480\n",
      "Epoch: 97 cost = 0.17422\n",
      "Epoch: 98 cost = 0.17934\n",
      "Epoch: 99 cost = 0.18190\n",
      "Epoch: 100 cost = 0.18982\n",
      "Validation Accuracy: 0.9354\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# splitting the data into train, val,test\n",
    "seed = 128\n",
    "rng = np.random.RandomState(seed)\n",
    "\n",
    "def one_hot_encoding(numClasses):\n",
    "    # producing one hot encoding\n",
    "    pass\n",
    "\n",
    "def preproc():\n",
    "    # converting values to 0-1\n",
    "    pass\n",
    "    \n",
    "def batchForm():\n",
    "    # form the batch of given batch size\n",
    "    pass\n",
    "\n",
    "input_num_units = 784\n",
    "hidden_num_units = 20\n",
    "output_num_units = 10\n",
    "\n",
    "# define placeholders, i.e. way to feed values to computational graph\n",
    "x = tf.placeholder(tf.float32, [None, input_num_units])\n",
    "y = tf.placeholder(tf.float32, [None, output_num_units])\n",
    "\n",
    "# set remaining parameters\n",
    "epochs = 100\n",
    "# batch_size = 128\n",
    "learning_rate = 0.01\n",
    "\n",
    "weights = {\n",
    "    'hidden': tf.Variable(tf.random_normal([input_num_units, hidden_num_units], seed=seed)),\n",
    "    'output': tf.Variable(tf.random_normal([hidden_num_units, output_num_units], seed=seed))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'hidden': tf.Variable(tf.random_normal([hidden_num_units], seed=seed)),\n",
    "    'output': tf.Variable(tf.random_normal([output_num_units], seed=seed))\n",
    "}\n",
    "\n",
    "hidden_layer = tf.add(tf.matmul(x, weights['hidden']), biases['hidden'])\n",
    "hidden_layer = tf.nn.relu(hidden_layer)\n",
    "\n",
    "output_layer = tf.matmul(hidden_layer, weights['output']) + biases['output']\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=output_layer, labels=y))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "init_op = tf.initialize_all_variables()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # create initialized variables\n",
    "    sess.run(init)        \n",
    "    for epoch in range(epochs):\n",
    "        avg_cost = 0\n",
    "        total_batch = 55\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = mnist.train.next_batch(200)\n",
    "            _, c = sess.run([optimizer, cost], feed_dict = {x: batch_x, y: batch_y})\n",
    "            avg_cost += c / total_batch \n",
    "\n",
    "        print(\"Epoch:\", (epoch+1), \"cost =\", \"{:.5f}\".format(avg_cost))\n",
    "    \n",
    "    pred_temp = tf.equal(tf.argmax(output_layer, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(pred_temp, \"float\"))\n",
    "    print(\"Testing Accuracy:\", accuracy.eval({x: val_x.reshape(-1, 784), y: val_y}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
