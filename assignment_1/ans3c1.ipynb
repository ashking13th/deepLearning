{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "numx = 0\n",
    "\n",
    "def make_data():\n",
    "    \n",
    "\timageData=[]\n",
    "\tlabels=[]\n",
    "\tdata=[]\n",
    "\tfeatures=28*28*3\n",
    "\tnoImage=96000\n",
    "\tnClass=96\n",
    "\tpath=os.getcwd()\n",
    "\tfolderpath=os.path.join(path,\"classes\")\n",
    "\tvidpath=os.path.join(path,\"vimage\")\n",
    "\n",
    "\tlabel=-1\n",
    "\tfor length in range(2):\n",
    "\t\tfor width in range(2):\n",
    "\t\t\tfor angle in range(12):\n",
    "\t\t\t\tfor color in range(2):\n",
    "\n",
    "\t\t\t\t\tstring=str(length)+\"_\"+str(width)+\"_\"+str(angle)+\"_\"+str(color)\n",
    "\t\t\t\t\tfpath=folderpath+\"/\"+string\n",
    "\n",
    "\t\t\t\t\tipath=fpath+\"/\"+str(length)+\"_\"+str(width)+\"_\"+str(angle)+\"_\"+str(color)\n",
    "\t\t\t\t\tlabel=label+1\n",
    "\n",
    "\t\t\t\t\tfor image in range(1000):\n",
    "\t\t\t\t\t\tiName=ipath+\"_\"+str(image)+\".jpeg\"\n",
    "\t\t\t\t\t\timg_data=cv2.imread(iName)\n",
    "\t\t\t\t\t\tdata.append(img_data.flatten().reshape(features))\n",
    "\t\t\t\t\t\tlabels.append(label)\n",
    "\timageData=np.array(data,float).reshape(noImage,features)\n",
    "\tlabels=np.array(labels,float).reshape(noImage,1)\n",
    "\timageData=imageData/255.0\n",
    "\tdata=np.concatenate((imageData,labels),axis=1)\n",
    "\tnp.random.shuffle(data)\n",
    "\tdata=data.reshape(noImage,features+1)\n",
    "\n",
    "\n",
    "\tipt=data[:,:-1]\n",
    "\topt=np.array(data[:,-1],int).reshape(noImage,1)\n",
    "\tb=np.zeros((noImage,nClass),int)\n",
    "\tfor i in range(noImage):\n",
    "\t\tb[i,opt[i]]=1\n",
    "\n",
    "\treturn ipt,b\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96000, 2352) (96000, 96)\n"
     ]
    }
   ],
   "source": [
    "x,y=make_data()\n",
    "print(x.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx = x\n",
    "ny = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96000, 2352)\n",
      "(67200, 2352)\n",
      "Epoch: 1 cost = 5.27598\n",
      "Epoch: 2 cost = 4.34517\n",
      "Epoch: 3 cost = 3.93747\n",
      "Epoch: 4 cost = 3.35672\n",
      "Epoch: 5 cost = 2.88567\n",
      "Epoch: 6 cost = 2.52759\n",
      "Epoch: 7 cost = 2.21045\n",
      "Epoch: 8 cost = 1.96234\n",
      "Epoch: 9 cost = 1.76791\n",
      "Epoch: 10 cost = 1.57654\n",
      "Epoch: 11 cost = 1.41582\n",
      "Epoch: 12 cost = 1.27969\n",
      "Epoch: 13 cost = 1.14249\n",
      "Epoch: 14 cost = 1.00717\n",
      "Epoch: 15 cost = 0.90844\n",
      "Epoch: 16 cost = 0.84146\n",
      "Epoch: 17 cost = 0.78307\n",
      "Epoch: 18 cost = 0.73174\n",
      "Epoch: 19 cost = 0.68239\n",
      "Epoch: 20 cost = 0.64432\n",
      "Epoch: 21 cost = 0.61038\n",
      "Epoch: 22 cost = 0.57895\n",
      "Epoch: 23 cost = 0.55472\n",
      "Epoch: 24 cost = 0.52740\n",
      "Epoch: 25 cost = 0.50817\n",
      "Epoch: 26 cost = 0.48917\n",
      "Epoch: 27 cost = 0.47051\n",
      "Epoch: 28 cost = 0.45897\n",
      "Epoch: 29 cost = 0.45112\n",
      "Epoch: 30 cost = 0.43834\n",
      "Epoch: 31 cost = 0.42164\n",
      "Epoch: 32 cost = 0.40938\n",
      "Epoch: 33 cost = 0.40122\n",
      "Epoch: 34 cost = 0.39199\n",
      "Epoch: 35 cost = 0.38483\n",
      "Epoch: 36 cost = 0.37784\n",
      "Epoch: 37 cost = 0.36614\n",
      "Epoch: 38 cost = 0.35958\n",
      "Epoch: 39 cost = 0.35093\n",
      "Epoch: 40 cost = 0.34788\n",
      "Epoch: 41 cost = 0.33487\n",
      "Epoch: 42 cost = 0.32846\n",
      "Epoch: 43 cost = 0.33041\n",
      "Epoch: 44 cost = 0.31946\n",
      "Epoch: 45 cost = 0.30846\n",
      "Epoch: 46 cost = 0.31186\n",
      "Epoch: 47 cost = 0.29890\n",
      "Epoch: 48 cost = 0.31504\n",
      "Epoch: 49 cost = 0.29171\n",
      "Epoch: 50 cost = 0.27822\n",
      "Epoch: 51 cost = 0.28632\n",
      "Epoch: 52 cost = 0.28904\n",
      "Epoch: 53 cost = 0.26914\n",
      "Epoch: 54 cost = 0.27294\n",
      "Epoch: 55 cost = 0.26060\n",
      "Epoch: 56 cost = 0.26367\n",
      "Epoch: 57 cost = 0.27705\n",
      "Epoch: 58 cost = 0.25217\n",
      "Epoch: 59 cost = 0.24823\n",
      "Epoch: 60 cost = 0.24626\n",
      "Epoch: 61 cost = 0.23755\n",
      "Epoch: 62 cost = 0.24727\n",
      "Epoch: 63 cost = 0.24728\n",
      "Epoch: 64 cost = 0.23477\n",
      "Epoch: 65 cost = 0.23056\n",
      "Epoch: 66 cost = 0.22729\n",
      "Epoch: 67 cost = 0.22044\n",
      "Epoch: 68 cost = 0.22853\n",
      "Epoch: 69 cost = 0.22060\n",
      "Epoch: 70 cost = 0.20634\n",
      "Epoch: 71 cost = 0.21130\n",
      "Epoch: 72 cost = 0.22492\n",
      "Epoch: 73 cost = 0.20562\n",
      "Epoch: 74 cost = 0.21635\n",
      "Epoch: 75 cost = 0.21239\n",
      "Epoch: 76 cost = 0.19538\n",
      "Epoch: 77 cost = 0.19444\n",
      "Epoch: 78 cost = 0.19232\n",
      "Epoch: 79 cost = 0.18957\n",
      "Epoch: 80 cost = 0.20950\n",
      "Epoch: 81 cost = 0.18787\n",
      "Epoch: 82 cost = 0.17991\n",
      "Epoch: 83 cost = 0.18460\n",
      "Epoch: 84 cost = 0.18897\n",
      "Epoch: 85 cost = 0.17205\n",
      "Epoch: 86 cost = 0.18778\n",
      "Epoch: 87 cost = 0.17751\n",
      "Epoch: 88 cost = 0.16887\n",
      "Epoch: 89 cost = 0.17568\n",
      "Epoch: 90 cost = 0.16933\n",
      "Epoch: 91 cost = 0.15963\n",
      "Epoch: 92 cost = 0.19260\n",
      "Epoch: 93 cost = 0.15347\n",
      "Epoch: 94 cost = 0.15401\n",
      "Epoch: 95 cost = 0.16509\n",
      "Epoch: 96 cost = 0.15530\n",
      "Epoch: 97 cost = 0.16405\n",
      "Epoch: 98 cost = 0.18526\n",
      "Epoch: 99 cost = 0.14007\n",
      "Epoch: 100 cost = 0.16104\n",
      "Testing Accuracy: 0.9389236\n"
     ]
    }
   ],
   "source": [
    "x = nx\n",
    "y = ny\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "print(x.shape)\n",
    "split_size = int(x.shape[0]*0.7)\n",
    "\n",
    "x, val_x = x[:split_size,:], x[split_size:,:]\n",
    "y, val_y = y[:split_size,:], y[split_size:,:]\n",
    "\n",
    "print(x.shape)\n",
    "\n",
    "batchPtr = 0\n",
    "# splitting the data into train, val,test\n",
    "seed = 128\n",
    "rng = np.random.RandomState(seed)\n",
    "\n",
    "def one_hot_encoding(numClasses):\n",
    "    # producing one hot encoding\n",
    "    pass\n",
    "\n",
    "def preproc():\n",
    "    # converting values to 0-1\n",
    "    pass\n",
    "    \n",
    "def batchForm():\n",
    "    # form the batch of given batch size\n",
    "    global batchPtr\n",
    "    batch_x = x[batchPtr:batchPtr+batchSize,:]\n",
    "    batch_y = y[batchPtr:batchPtr+batchSize,:]\n",
    "    batchPtr += batchSize\n",
    "#     print(batch_x)\n",
    "#     print(batch_y)\n",
    "    return batch_x,batch_y\n",
    "    \n",
    "\n",
    "input_num_units = 784*3\n",
    "hidden_num_units = 10\n",
    "output_num_units = 96\n",
    "batchSize = 200\n",
    "\n",
    "# placeholder to feed values during \n",
    "x1 = tf.placeholder(tf.float32, [None, input_num_units])\n",
    "y1 = tf.placeholder(tf.float32, [None, output_num_units])\n",
    "\n",
    "# set remaining parameters\n",
    "epochs = 100\n",
    "learning_rate = 0.01\n",
    "\n",
    "weights = {\n",
    "    'hidden': tf.Variable(tf.random_normal([input_num_units, hidden_num_units], seed=seed)),\n",
    "    'output': tf.Variable(tf.random_normal([hidden_num_units, output_num_units], seed=seed))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'hidden': tf.Variable(tf.random_normal([hidden_num_units], seed=seed)),\n",
    "    'output': tf.Variable(tf.random_normal([output_num_units], seed=seed))\n",
    "}\n",
    "\n",
    "hidden_layer = tf.add(tf.matmul(x1, weights['hidden']), biases['hidden'])\n",
    "hidden_layer = tf.nn.relu(hidden_layer)\n",
    "\n",
    "output_layer = tf.matmul(hidden_layer, weights['output']) + biases['output']\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=output_layer, labels=y1))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # create initialized variables\n",
    "    sess.run(init)        \n",
    "    for epoch in range(epochs):\n",
    "        avg_cost = 0\n",
    "        total_batch = int(x.shape[0] / batchSize)\n",
    "#         print(total_batch)\n",
    "#         print(x.shape[0])\n",
    "        batchPtr = 0\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = batchForm()\n",
    "#             print(batch_x)\n",
    "#             print(\"\\n\\n\\n\\n\")\n",
    "#             print(i)\n",
    "#             print(\"\\n\\n\\n\\n\")\n",
    "            _, c = sess.run([optimizer, cost], feed_dict = {x1: batch_x, y1: batch_y})\n",
    "            avg_cost += c / total_batch \n",
    "\n",
    "        print(\"Epoch:\", (epoch+1), \"cost =\", \"{:.5f}\".format(avg_cost))\n",
    "    \n",
    "    pred_temp = tf.equal(tf.argmax(output_layer, 1), tf.argmax(y1, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(pred_temp, \"float\"))\n",
    "    print(\"Testing Accuracy:\", accuracy.eval({x1: val_x.reshape(-1, 784*3), y1: val_y}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
